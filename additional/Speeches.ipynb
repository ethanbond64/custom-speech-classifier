{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = []\n",
    "for title in ['MLK','JFK','FDR1','FDR2','RR']:\n",
    "    filename =  \"Speeches/\" + title + '.txt'\n",
    "    file = open(filename).read()\n",
    "    tokens = nltk.sent_tokenize(file)\n",
    "    for s in range(len(tokens)):\n",
    "        tokens[s] = nltk.word_tokenize(tokens[s])\n",
    "    speeches.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0 27.0\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for speech in speeches:\n",
    "    for sent in speech:\n",
    "        lengths.append(len(sent))\n",
    "\n",
    "lengths = np.array(lengths)\n",
    "\n",
    "lower = np.percentile(lengths,35)\n",
    "upper = np.percentile(lengths,70)\n",
    "print(lower,upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_pos = []\n",
    "speeches_syl = []\n",
    "for speech in speeches: \n",
    "    speech_pos = []\n",
    "    speech_syl = []\n",
    "    for sent in speech:\n",
    "        sent_pos = nltk.pos_tag(sent)\n",
    "        sent_pos = [i[1] for i in sent_pos]\n",
    "        speech_pos.append(sent_pos)\n",
    "        sent_syl = []\n",
    "        for word in sent:\n",
    "            sent_syl.append(syllable_count(word))\n",
    "        speech_syl.append(sent_syl)\n",
    "    speeches_pos.append(speech_pos)\n",
    "    speeches_syl.append(speech_syl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(sent1, sent2):\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>See similarity between speeches<\\h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLK JFK\n",
      "0.6220304846867513 0.4923005233198352 0.7339773058637489\n",
      "0.3006662279964215 0.20184171229467396 0.37648809523809523\n",
      "MLK FDR1\n",
      "0.6146667205967447 0.5410492707088581 0.6881994069902073\n",
      "0.32166032709134856 0.2406191369606004 0.3946428571428572\n",
      "MLK FDR2\n",
      "0.6450252338399329 0.5098634398112862 0.7893458546863694\n",
      "0.33822507518913714 0.27313211281543787 0.41483516483516486\n",
      "MLK RR\n",
      "0.5734285519920796 0.4331064711033647 0.7241515524761672\n",
      "0.3409956575121285 0.26188455008488964 0.39097744360902253\n",
      "JFK FDR1\n",
      "0.6673191519134841 0.603501755903317 0.7785276161796155\n",
      "0.316748177448371 0.2543604651162791 0.36923076923076925\n",
      "JFK FDR2\n",
      "0.576865842494059 0.47262234670390935 0.7555928390503563\n",
      "0.26630284013765604 0.19345238095238096 0.3106060606060606\n",
      "JFK RR\n",
      "0.5705532812408614 0.4922003191214408 0.7089744831949277\n",
      "0.27695813837984745 0.21465677179962894 0.33695652173913043\n",
      "FDR1 FDR2\n",
      "0.6356488471492326 0.5942045827688409 0.705835895986157\n",
      "0.3269996522838861 0.29962686567164176 0.3857104485629868\n",
      "FDR1 RR\n",
      "0.5379296145914807 0.45383466692774943 0.6776051961385094\n",
      "0.3120177219283854 0.22997416020671835 0.3796106557377049\n",
      "FDR2 RR\n",
      "0.5278362626316097 0.3726042892732846 0.6546390406262115\n",
      "0.29574094335450196 0.24064171122994654 0.34409937888198755\n"
     ]
    }
   ],
   "source": [
    "speakers = ['MLK','JFK','FDR1','FDR2','RR']\n",
    "for j in range(4):\n",
    "    for k in range(j+1,5):\n",
    "        s1 = speeches_pos[j]\n",
    "        s2 = speeches_pos[k]\n",
    "        sims = []\n",
    "        diffs = []\n",
    "        for i in range(20):\n",
    "            sims.append(similarity(s1[i],s2[i]))\n",
    "            diffs.append(difflib.SequenceMatcher(None,s1[i],s2[i]).ratio())\n",
    "        print(speakers[j],speakers[k])\n",
    "        print(np.mean(sims),np.percentile(sims,25),np.percentile(sims,75))\n",
    "        print(np.mean(diffs),np.percentile(diffs,25),np.percentile(diffs,75))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"yellow\"> NOW INPUT TEXT AND SEE HOW SPEECHLIKE IT IS (POS Similarity) <\\font><\\h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon Score\n",
      "0.23845892696050544 0.19309686496050005 0.2806807813100855\n",
      "POS score\n",
      "0.6656616664554945 0.6323904763031278 0.7155120201934456\n",
      "Syllable scores\n",
      "0.5330082241333999 0.5008489488674571 0.5710753141448655\n",
      "[0.23845892696050544, 0.19309686496050005, 0.2806807813100855, 0.6656616664554945, 0.6323904763031278, 0.7155120201934456, 0.5330082241333999, 0.5008489488674571, 0.5710753141448655]\n"
     ]
    }
   ],
   "source": [
    "# take the input speech\n",
    "file = open('nonSpeeches/GlobalStudiesCorpus.txt').read()\n",
    "# file = open('Speeches/911.txt').read()\n",
    "inputSpeech = nltk.sent_tokenize(file)\n",
    "for s in range(len(inputSpeech)):\n",
    "    inputSpeech[s] = nltk.word_tokenize(inputSpeech[s])\n",
    "\n",
    "pos_inputSpeech = [] \n",
    "syl_inputSpeech = []\n",
    "\n",
    "for sent in inputSpeech:\n",
    "    sent_pos = nltk.pos_tag(sent)\n",
    "    sent_pos = [i[1] for i in sent_pos]\n",
    "    pos_inputSpeech.append(sent_pos)\n",
    "    sent_syl = []\n",
    "    for word in sent:\n",
    "        sent_syl.append(syllable_count(word))\n",
    "    syl_inputSpeech.append(sent_syl)\n",
    "        \n",
    "# loop through each sentence\n",
    "lexCompare = []\n",
    "posCompare = []\n",
    "sylCompare = []\n",
    "\n",
    "for sent in range(len(inputSpeech)):\n",
    "    \n",
    "    if len(inputSpeech[sent]) <= lower:\n",
    "        percentile = 0\n",
    "    elif len(inputSpeech[sent]) < upper:\n",
    "        percentile = 1\n",
    "    else:\n",
    "        percentile = 2\n",
    "\n",
    "    allLex = []\n",
    "    allPos = []\n",
    "    allSyl = []    \n",
    "    \n",
    "    for speech in range(len(speeches)):\n",
    "        for sent2 in range(len(speeches[speech])):\n",
    "            if percentile == 0:\n",
    "                if len(speeches_pos[speech][sent2]) <= lower:\n",
    "                    allLex.append(similarity(inputSpeech[sent],speeches[speech][sent2]))\n",
    "                    allPos.append(similarity(pos_inputSpeech[sent],speeches_pos[speech][sent2]))\n",
    "#                     allPos.append(difflib.SequenceMatcher(None,pos_inputSpeech[sent],speeches_pos[speech][sent2]).ratio())\n",
    "                    allSyl.append(difflib.SequenceMatcher(None,syl_inputSpeech[sent],speeches_syl[speech][sent2]).ratio())\n",
    "            elif percentile == 1:\n",
    "                if len(speeches_pos[speech][sent2]) > lower and len(speeches_pos[speech][sent2]) < upper:\n",
    "                    allLex.append(similarity(inputSpeech[sent],speeches[speech][sent2]))\n",
    "                    allPos.append(similarity(pos_inputSpeech[sent],speeches_pos[speech][sent2]))\n",
    "#                     allPos.append(difflib.SequenceMatcher(None,pos_inputSpeech[sent],speeches_pos[speech][sent2]).ratio())\n",
    "                    allSyl.append(difflib.SequenceMatcher(None,syl_inputSpeech[sent],speeches_syl[speech][sent2]).ratio())\n",
    "            else:\n",
    "                if len(speeches_pos[speech][sent2]) >= upper:\n",
    "                    allLex.append(similarity(inputSpeech[sent],speeches[speech][sent2]))\n",
    "                    allPos.append(similarity(pos_inputSpeech[sent],speeches_pos[speech][sent2]))\n",
    "#                     allPos.append(difflib.SequenceMatcher(None,pos_inputSpeech[sent],speeches_pos[speech][sent2]).ratio())\n",
    "                    allSyl.append(difflib.SequenceMatcher(None,syl_inputSpeech[sent],speeches_syl[speech][sent2]).ratio())\n",
    "    \n",
    "    lexAvg = np.mean(allLex)\n",
    "    lexCompare.append(lexAvg)\n",
    "    \n",
    "    posAvg = np.mean(allPos)\n",
    "    posCompare.append(posAvg)\n",
    "    \n",
    "    sylAvg = np.mean(allSyl)\n",
    "    sylCompare.append(sylAvg)\n",
    "    \n",
    "print('Lexicon Score')\n",
    "print(np.mean(lexCompare),np.percentile(lexCompare,25),np.percentile(lexCompare,75))\n",
    "print('POS score')\n",
    "print(np.mean(posCompare),np.percentile(posCompare,25),np.percentile(posCompare,75))\n",
    "print('Syllable scores')\n",
    "print(np.mean(sylCompare),np.percentile(sylCompare,25),np.percentile(sylCompare,75))\n",
    "\n",
    "dataForML = [np.mean(lexCompare),np.percentile(lexCompare,25),np.percentile(lexCompare,75),\n",
    "            np.mean(posCompare),np.percentile(posCompare,25),np.percentile(posCompare,75),\n",
    "            np.mean(sylCompare),np.percentile(sylCompare,25),np.percentile(sylCompare,75)]\n",
    "print(dataForML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "Analyze more than just pos and pos sequences <<-- in the future we will use both\n",
    "What else to add:\n",
    "2.words themselves similarity\n",
    "1.syllables and syllable sequences\n",
    "3.size\n",
    "\n",
    "LBJ\n",
    "Lexicon Score\n",
    "0.2206770320260939 0.16444150300872848 0.26751448186390947\n",
    "POS score\n",
    "0.58712533150487 0.5050302214253439 0.6801564703521834\n",
    "Syllable scores\n",
    "0.5890299449491577 0.5561725977374865 0.6233668124636911\n",
    "\n",
    "\n",
    "911\n",
    "Lexicon Score\n",
    "0.21909331149281452 0.16822689873957603 0.2826479644797504\n",
    "POS score\n",
    "0.5679722102197743 0.4622933870068829 0.6686992907910609\n",
    "Syllable scores\n",
    "0.5533959611271846 0.5206761695675282 0.5981738554805685\n",
    "\n",
    "\n",
    "Steve Jobs\n",
    "Lexicon Score\n",
    "0.18889641447264988 0.1526232414666126 0.21799046271700828\n",
    "POS score\n",
    "0.512222183457417 0.4259241229403694 0.6103960559618823\n",
    "Syllable scores\n",
    "0.5890760120927805 0.5519041936890691 0.6362407520634856\n",
    "\n",
    "MLK (not compared to itself)\n",
    "Lexicon Score\n",
    "0.23360487633843435 0.17911676269336066 0.2808387317663956\n",
    "POS score\n",
    "0.6114183229340353 0.545222719091455 0.6988506486718966\n",
    "Syllable scores\n",
    "0.5780575744862335 0.5469768361534566 0.6084192133145911\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "take the three metrics as inputs and then use them as inputs for a ml classifier\n",
    "make a dash app that takes in the text on one side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color = \"red\"> APP STUFF </font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Training Data\n",
    "#order\n",
    "#LBJ\n",
    "#911\n",
    "#BSA\n",
    "#globalStudies\n",
    "#Medium\n",
    "#SethGodin\n",
    "\n",
    "X = [\n",
    "    [0.2206770320260939, 0.16444150300872848, 0.26751448186390947, 0.58712533150487, 0.5050302214253439, 0.6801564703521834, 0.5890299449491577, 0.5561725977374865, 0.6233668124636911],\n",
    "    [0.21909331149281452, 0.16822689873957603, 0.2826479644797504, 0.5679722102197743, 0.4622933870068829, 0.6686992907910609, 0.5533959611271846, 0.5206761695675282, 0.5981738554805685],\n",
    "    [0.18639193230159604, 0.13879495860288443, 0.20927516051956438, 0.5782069232444341, 0.5024750410242014, 0.6583653027793185, 0.5858910942585438, 0.5664235386626175, 0.6131445223876603],\n",
    "    [0.22479072982187454, 0.18636712941809297, 0.26951768909072904, 0.6577229024142464, 0.5874365398901084, 0.7356621954845705, 0.5518841529444279, 0.5249945243930422, 0.5796050147073137],\n",
    "#     [0.17064183606190358, 0.13184682346106283, 0.20599061296683502, 0.49838191720146113, 0.3965624149541385, 0.5987464416853956, 0.5680601664983725, 0.5259715344172728, 0.627135788291258],\n",
    "    [0.1848718815401489, 0.13693315333732198, 0.2102304448015972, 0.5141976142890523, 0.4084460467781959, 0.6410679832199497, 0.6078637832180149, 0.5805887283151228, 0.6523637058160225]\n",
    "]\n",
    "\n",
    "y = [1.0,1.0,0.0,0.0,0.0]\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=None, random_state=720)\n",
    "clf.fit(X, y)\n",
    "clf.score(X,y)\n",
    "clf.predict([[0.17064183606190358, 0.13184682346106283, 0.20599061296683502, 0.49838191720146113, 0.3965624149541385, 0.5987464416853956, 0.5680601664983725, 0.5259715344172728, 0.627135788291258]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
